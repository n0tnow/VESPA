#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
from bs4 import BeautifulSoup
import json
import time
import re
import hashlib
import logging
import os
from pathlib import Path
from urllib.parse import urljoin

# Logging ayarlarÄ±
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class VespaScraper:
    def __init__(self, download_images=False):
        self.base_url = "https://www.motopit.com.tr"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.parts_data = {}
        self.models_data = {}
        self.download_images = download_images
        
        if download_images:
            Path("images").mkdir(exist_ok=True)
    
    def get_page(self, url, max_retries=3):
        """Sayfa iÃ§eriÄŸini gÃ¼venli ÅŸekilde Ã§eker"""
        for attempt in range(max_retries):
            try:
                response = self.session.get(url, timeout=15)
                response.raise_for_status()
                return response
            except Exception as e:
                logger.warning(f"Attempt {attempt + 1} failed for {url}: {e}")
                if attempt < max_retries - 1:
                    time.sleep(2)
                else:
                    logger.error(f"Failed to get {url} after {max_retries} attempts")
                    return None
    
    def parse_price(self, price_text):
        """Fiyat metnini sayÄ±sal deÄŸere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r"""
        if not price_text:
            return None
        
        logger.debug(f"Parsing price: '{price_text}'")
        
        # TL kelimesini kaldÄ±r
        clean_price = price_text.replace('TL', '').strip()
        
        # TÃ¼rkÃ§e format: 2.375,50 -> 2375.50
        if ',' in clean_price and '.' in clean_price:
            # Hem binlik hem ondalÄ±k var: 2.375,50
            parts = clean_price.split(',')
            if len(parts) == 2:
                integer_part = parts[0].replace('.', '')  # Binlik ayraÃ§larÄ± kaldÄ±r
                decimal_part = parts[1]
                clean_price = f"{integer_part}.{decimal_part}"
        elif ',' in clean_price:
            # Sadece ondalÄ±k: 375,50
            clean_price = clean_price.replace(',', '.')
        elif '.' in clean_price and len(clean_price.split('.')[-1]) > 2:
            # Muhtemelen binlik ayracÄ±: 2.375
            clean_price = clean_price.replace('.', '')
        
        try:
            result = float(clean_price)
            logger.debug(f"Parsed price: {result}")
            return result
        except Exception as e:
            logger.debug(f"Price parsing failed: {e}")
            return None
    
    def get_product_images(self, product_name, soup):
        """ÃœrÃ¼n resim linklerini Ã§eker"""
        images = {
            'thumbnail': None,
            'main': None,
            'gallery': []
        }
        
        # BeautifulSoup ile resim linklerini bul
        img_tags = soup.find_all('img')
        
        for img in img_tags:
            src = img.get('src') or img.get('data-src')
            if src:
                # Tam URL yap
                if src.startswith('/'):
                    src = self.base_url + src
                elif not src.startswith('http'):
                    src = self.base_url + '/' + src
                
                # Resim tipini belirle
                if any(keyword in src.lower() for keyword in ['thumb', 'small', 'list']):
                    if not images['thumbnail']:
                        images['thumbnail'] = src
                elif any(keyword in src.lower() for keyword in ['main', 'large', 'detail']):
                    if not images['main']:
                        images['main'] = src
                elif any(keyword in src.lower() for keyword in ['product', 'item', 'vespa']):
                    # Ana resim yoksa ilk uygun resmi ana resim yap
                    if not images['main']:
                        images['main'] = src
                    # Galeri olarak da ekle
                    if src not in images['gallery'] and len(images['gallery']) < 3:
                        images['gallery'].append(src)
        
        return images
    
    def generate_part_id(self, name, price):
        """ÃœrÃ¼n iÃ§in benzersiz ID oluÅŸturur"""
        text = f"{name}_{price}".encode('utf-8')
        return hashlib.md5(text).hexdigest()[:8]
    
    def extract_products_from_text(self, html_content):
        """HTML iÃ§eriÄŸinden Ã¼rÃ¼nleri Ã§Ä±karÄ±r"""
        products = []
        
        # HTML'i temizle ve sadece text al
        soup = BeautifulSoup(html_content, 'html.parser')
        text_content = soup.get_text()
        
        logger.debug(f"HTML text Ã§Ä±karÄ±ldÄ±, uzunluk: {len(text_content)}")
        
        # TÃ¼m Ã¼rÃ¼n verisinin bulunduÄŸu uzun satÄ±rÄ± bul
        product_data_line = None
        for line in text_content.split('\n'):
            if 'MTP' in line and 'HÄ±zlÄ± Ä°ncele' in line and len(line) > 1000:
                product_data_line = line
                logger.debug(f"ÃœrÃ¼n verisi bulundu, uzunluk: {len(line)}")
                break
        
        if not product_data_line:
            logger.warning("ÃœrÃ¼n verisi iÃ§eren uzun satÄ±r bulunamadÄ±!")
            return products
        
        # MTP kodlarÄ± ile Ã¼rÃ¼nleri ayÄ±r
        mtp_pattern = r'(MTP\d+)'
        segments = re.split(mtp_pattern, product_data_line)
        
        logger.debug(f"MTP pattern ile {len(segments)} segment bulundu")
        
        i = 1  # Ä°lk segment genellikle header, 1'den baÅŸla
        while i < len(segments) - 1:
            mtp_code = segments[i]
            product_segment = segments[i + 1]
            
            if mtp_code.startswith('MTP'):
                logger.debug(f"Ä°ÅŸleniyor: {mtp_code} -> {product_segment[:100]}...")
                
                # Bu segment'ten Ã¼rÃ¼n bilgilerini Ã§Ä±kar
                product = self.parse_product_segment(mtp_code, product_segment)
                if product:
                    products.append(product)
                    logger.debug(f"âœ… ÃœrÃ¼n eklendi: {product['name']} - {product['price']}TL")
            
            i += 2
        
        logger.info(f"Toplam {len(products)} Ã¼rÃ¼n bulundu")
        return products
    
    def parse_product_segment(self, mtp_code, segment):
        """Tek bir Ã¼rÃ¼n segmentini parse eder"""
        try:
            # Fiyat pattern'i: Tam fiyatÄ± yakala - 12.467,00TL formatÄ±nda
            price_patterns = [
                r'(\d{1,2}\.\d{3},\d{2})TL',  # 12.467,00TL formatÄ±
                r'(\d{1,3},\d{2})TL',         # 467,00TL formatÄ±  
                r'(\d+)TL'                    # 467TL formatÄ±
            ]
            
            price_match = None
            price_text = None
            
            for pattern in price_patterns:
                matches = re.findall(pattern, segment)
                if matches:
                    # Ä°lk bulduÄŸu fiyatÄ± al
                    price_text = matches[0] + "TL"
                    logger.debug(f"Fiyat bulundu {mtp_code}: {price_text}")
                    break
            
            if not price_text:
                logger.debug(f"Fiyat bulunamadÄ±: {mtp_code}")
                return None
            
            price = self.parse_price(price_text)
            
            if not price or price <= 0:
                logger.debug(f"GeÃ§ersiz fiyat: {mtp_code} - {price_text}")
                return None
            
            # ÃœrÃ¼n adÄ±nÄ± bul - MTP kodundan sonra ilk kelime grubu
            # Pattern: MTP0514749Aks RulmanÄ± .HÄ±zlÄ± Ä°ncele
            name_pattern = r'^([^.]+)\.?HÄ±zlÄ± Ä°ncele'
            name_match = re.search(name_pattern, segment)
            
            if name_match:
                product_name = name_match.group(1).strip()
            else:
                # Alternatif: ilk noktaya kadar olan kÄ±sÄ±m
                parts = segment.split('.')[0].strip()
                product_name = parts
            
            # ÃœrÃ¼n adÄ±nÄ± temizle
            product_name = self.clean_product_name(product_name)
            
            if not self.is_valid_product(product_name, price):
                logger.debug(f"GeÃ§ersiz Ã¼rÃ¼n: {product_name}")
                return None
            
            part_id = self.generate_part_id(product_name, price)
            
            # Resim linklerini Ã§ek (ÅŸimdilik boÅŸ)
            image_urls = {
                'thumbnail': None,
                'main': None,
                'gallery': []
            }
            
            return {
                'id': part_id,
                'name': product_name,
                'price': price,
                'url': self.generate_product_url(product_name),
                'images': image_urls,
                'mtp_code': mtp_code
            }
            
        except Exception as e:
            logger.debug(f"Segment parse hatasÄ± {mtp_code}: {e}")
            return None
    
    def clean_product_name(self, name):
        """ÃœrÃ¼n adÄ±nÄ± temizler"""
        if not name:
            return ""
        
        # BaÅŸtaki gereksiz karakterleri temizle
        while name and name[0] in '-â€¢*0123456789. ':
            name = name[1:]
        
        # Sondaki gereksiz kÄ±sÄ±mlarÄ± temizle
        name = name.replace('HÄ±zlÄ± Ä°ncele', '').strip()
        
        # Ã‡ift boÅŸluklarÄ± tek boÅŸluk yap
        name = re.sub(r'\s+', ' ', name)
        
        # Ã–zel karakterleri temizle ama TÃ¼rkÃ§e karakterleri koru
        name = re.sub(r'[^\w\sÃ‡Ã§ÄžÄŸÄ°Ä±Ã–Ã¶ÅžÅŸÃœÃ¼/()-]', '', name)
        
        return name.strip()
    
    def is_valid_product(self, name, price):
        """ÃœrÃ¼nÃ¼n geÃ§erli olup olmadÄ±ÄŸÄ±nÄ± kontrol eder"""
        if not name or len(name) < 3:
            return False
        
        if not price or price <= 0:
            return False
        
        # Ã‡ok temel filtre - sadece aÃ§Ä±k sistem kelimelerini atla
        skip_keywords = [
            'menu', 'kategori', 'arama', 'sepet', 'hesap', 
            'ucretsiz kargo', 'aracinizi secin', 'alisveris',
            'giris', 'uye ol', 'taksit', 'odeme'
        ]
        
        name_lower = name.lower()
        for keyword in skip_keywords:
            if keyword in name_lower:
                logger.debug(f"Filtered out: {name} (contains: {keyword})")
                return False
        
        return True
    
    def generate_product_url(self, name):
        """ÃœrÃ¼n adÄ±ndan URL oluÅŸturur"""
        clean_name = re.sub(r'[^a-zA-Z0-9\s]', '', name.lower())
        slug = '-'.join(clean_name.split())
        return f"{self.base_url}/urun/{slug}"
    
    def get_vespa_models(self):
        """Vespa modellerini Ã§eker"""
        logger.info("Vespa modelleri Ã§ekiliyor...")
        
        # Test ve production modlarÄ±
        test_models = [
            {
                'name': 'Vespa Primavera 150 3v',
                'url': 'https://www.motopit.com.tr/kategori/vespa-primavera-150-3v-yedek-parca'
            },
            {
                'name': 'Vespa GTS 300',
                'url': 'https://www.motopit.com.tr/kategori/vespa-gts-300-yedek-parca'
            },
            {
                'name': 'Vespa Sprint 125',
                'url': 'https://www.motopit.com.tr/kategori/vespa-sprint-125-3v-ie-yedek-parca'
            }
        ]
        
        choice = input("\nKaÃ§ model iÅŸlensin? (1=test, 3=Ã§oklu test, 0=tÃ¼mÃ¼): ").strip()
        
        if choice == "1":
            models = test_models[:1]
            logger.info(f"Test modu: {len(models)} model")
        elif choice == "3":
            models = test_models[:3]  
            logger.info(f"Ã‡oklu test modu: {len(models)} model")
        else:
            # TÃ¼m modelleri dinamik olarak Ã§ek
            models = self.get_all_vespa_models()
            if not models:
                logger.warning("Dinamik model bulunamadÄ±, test modellerini kullanÄ±yorum")
                models = test_models
                
        for i, model in enumerate(models):
            logger.info(f"  Model {i+1}: {model['name']}")
            
        return models
    
    def get_all_vespa_models(self):
        """TÃ¼m Vespa modellerini dinamik olarak Ã§eker"""
        try:
            main_url = f"{self.base_url}/kategori/vespa-yedek-parca"
            response = self.get_page(main_url)
            
            if not response:
                return []
                
            # Basit model listesi - geniÅŸletilebilir
            known_models = [
                ('primavera-150-3v', 'Vespa Primavera 150 3v'),
                ('gts-300', 'Vespa GTS 300'),  
                ('sprint-125-3v-ie', 'Vespa Sprint 125'),
                ('et4-150', 'Vespa ET4 150'),
                ('lx-150-ie', 'Vespa LX 150'),
                ('gts-250', 'Vespa GTS 250'),
                ('primavera-125', 'Vespa Primavera 125')
            ]
            
            models = []
            for slug, name in known_models:
                url = f"{self.base_url}/kategori/vespa-{slug}-yedek-parca"
                models.append({'name': name, 'url': url})
                
            return models
            
        except Exception as e:
            logger.error(f"Model Ã§ekme hatasÄ±: {e}")
            return []
    
    def extract_model_name_from_url(self, url):
        """URL'den model adÄ±nÄ± Ã§Ä±karÄ±r"""
        try:
            # /kategori/vespa-primavera-150-3v-yedek-parca -> Vespa Primavera 150 3v
            parts = url.split('/')[-1]
            parts = parts.replace('-yedek-parca', '')
            parts = parts.replace('vespa-', '')
            
            # Kelimeler arasÄ± tire -> boÅŸluk
            model_name = parts.replace('-', ' ').title()
            return f"Vespa {model_name}"
        except:
            return None
    
    def scrape_model_products(self, model_url, model_name):
        """Belirli bir modelin Ã¼rÃ¼nlerini Ã§eker"""
        logger.info(f"Model Ã§ekiliyor: {model_name}")
        
        response = self.get_page(model_url)
        if not response:
            return []
        
        # HTML iÃ§eriÄŸini parse et
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ÃœrÃ¼nleri Ã§Ä±kar
        products = self.extract_products_from_text(response.content)
        
        # Her Ã¼rÃ¼n iÃ§in resim linklerini Ã§ek
        for product in products:
            # Resim linklerini Ã§ek (mevcut fonksiyonu kullan)
            if self.download_images:
                product['images'] = self.get_product_images(product['name'], soup)
            else:
                # Sadece HTML'den basit resim linklerini Ã§ek
                product['images'] = self.get_basic_image_links(soup)
            
            # Model bilgisini ekle
            product['model'] = model_name
        
        logger.info(f"{model_name}: {len(products)} Ã¼rÃ¼n bulundu")
        return products
    
    def get_basic_image_links(self, soup):
        """HTML'den basit resim linklerini Ã§eker"""
        images = {
            'thumbnail': None,
            'main': None, 
            'gallery': []
        }
        
        # TÃ¼m img taglarÄ±nÄ± bul
        img_tags = soup.find_all('img', src=True)
        
        for img in img_tags[:3]:  # Ä°lk 3 resmi al
            src = img['src']
            
            # Tam URL yap
            if src.startswith('/'):
                src = self.base_url + src
            elif not src.startswith('http'):
                src = self.base_url + '/' + src
            
            # Ä°lk resim thumbnail, ikinci main, diÄŸerleri gallery
            if not images['thumbnail']:
                images['thumbnail'] = src
            elif not images['main']:
                images['main'] = src
            else:
                if len(images['gallery']) < 2:
                    images['gallery'].append(src)
        
        return images
    
    def save_data(self):
        """Verileri JSON dosyalarÄ±na kaydeder"""
        # parts.json
        parts_output = {
            'parts': {part['id']: {
                'name': part['name'],
                'category': 'Genel',
                'price': part['price'],
                'url': part['url'],
                'images': part['images']
            } for part in self.parts_data.values()},
            'total_parts': len(self.parts_data),
            'generated_at': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        
        with open('parts.json', 'w', encoding='utf-8') as f:
            json.dump(parts_output, f, ensure_ascii=False, indent=2)
        
        # models.json
        models_output = {
            'models': self.models_data,
            'total_models': len(self.models_data),
            'generated_at': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        
        with open('models.json', 'w', encoding='utf-8') as f:
            json.dump(models_output, f, ensure_ascii=False, indent=2)
        
        logger.info("Veriler kaydedildi: parts.json, models.json")
    
    def run(self):
        """Ana Ã§alÄ±ÅŸtÄ±rma fonksiyonu"""
        try:
            logger.info("Vespa yedek parÃ§a scraper baÅŸlatÄ±lÄ±yor...")
            
            # Modelleri Ã§ek
            models = self.get_vespa_models()
            
            if not models:
                logger.error("HiÃ§ model bulunamadÄ±!")
                return
            
            total_parts = 0
            
            # Her model iÃ§in Ã¼rÃ¼nleri Ã§ek
            for model in models:
                model_name = model['name']
                model_url = model['url']
                
                # Model Ã¼rÃ¼nlerini Ã§ek
                products = self.scrape_model_products(model_url, model_name)
                
                # Verileri organize et
                model_part_ids = []
                
                for product in products:
                    part_id = product['id']
                    
                    # Parts verisine ekle (eÄŸer yoksa)
                    if part_id not in self.parts_data:
                        self.parts_data[part_id] = product
                        total_parts += 1
                    
                    model_part_ids.append(part_id)
                
                # Model verisini kaydet
                self.models_data[model_name] = {
                    'categories': {
                        'Genel': {
                            'parts': model_part_ids,
                            'url': model_url
                        }
                    },
                    'url': model_url
                }
                
                # Rate limiting
                time.sleep(2)
            
            # SonuÃ§larÄ± kaydet
            self.save_data()
            
            # Ã–zet
            logger.info(f"TamamlandÄ±! Toplam {total_parts} Ã¼rÃ¼n bulundu")
            
            print(f"\n=== Ã–ZET ===")
            print(f"Ä°ÅŸlenen model sayÄ±sÄ±: {len(models)}")
            print(f"Toplam Ã¼rÃ¼n sayÄ±sÄ±: {total_parts}")
            
            # Ã–rnek Ã¼rÃ¼nler
            if self.parts_data:
                print(f"\nÃ–rnek Ã¼rÃ¼nler:")
                for i, (part_id, part) in enumerate(list(self.parts_data.items())[:5]):
                    print(f"  {i+1}. {part['name']} - {part['price']}TL")
            
        except Exception as e:
            logger.error(f"Hata oluÅŸtu: {e}")
            raise

def main():
    print("ðŸï¸  === Vespa Yedek ParÃ§a Scraper === ðŸï¸")
    print("Motopit.com.tr sitesinden Vespa yedek parÃ§a verilerini Ã§eker")
    print("âœ… Fiyat parsing dÃ¼zeltildi")
    print("âœ… Resim link Ã§ekme aktif") 
    print("âœ… Multiple model desteÄŸi")
    print()
    
    # Debug seviyesi
    debug_mode = input("Debug modu aktif olsun mu? (y/n): ").lower() in ['y', 'yes', 'evet']
    if debug_mode:
        logging.getLogger().setLevel(logging.DEBUG)
        print("ðŸ” Debug modu aktif - detaylÄ± loglar gÃ¶sterilecek")
    
    # Resim seÃ§eneÄŸi  
    download_images = input("Resim indirmek istiyor musunuz? (y/n): ").lower() in ['y', 'yes', 'evet']
    
    if download_images:
        print("ðŸ“¥ ÃœrÃ¼n bilgileri Ã§ekilecek ve resimler indirilecek")
    else:
        print("ðŸ”— Sadece Ã¼rÃ¼n bilgileri ve resim linkleri Ã§ekilecek")
    
    # Scraper'Ä± baÅŸlat
    scraper = VespaScraper(download_images=download_images)
    scraper.run()

if __name__ == "__main__":
    main()